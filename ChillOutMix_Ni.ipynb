{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1zgnsJiJbY8481bxXAGIUgZlTl5-WyTJR",
      "authorship_tag": "ABX9TyPSG2r/MnE+8KW3B9gQJvCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iyashinouta/chilloutmix-auto1111-colab/blob/main/ChillOutMix_Ni.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://visitor-badge.glitch.me/badge?page_id=iyashinouta.cag-webui)\n",
        "\n",
        "Welcome to **[Stable Diffusion](https://github.com/Iyashinouta/chilloutmix-auto1111-colab)** from **Google Colab**\n",
        "\n",
        "<details>\n",
        "  <summary>Follow the instructions, it's simple</summary>\n",
        "\n",
        "1. You just click one by one, sequentially from top to bottom\n",
        "\n",
        "2. after the link appears on ***Launch UI***\n",
        "\n",
        "    click one (as you like) the link, and good luck\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "  <summary>P.S.</summary> \n",
        "\n",
        "* if U have any Issues, Report here\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/Github--white?style=for-the-badge&labelColor=gray&logo=github)](https://github.com/Iyashinouta/chilloutmix-auto1111-colab/issues)\n",
        "\n",
        "* or if U have any Questions, Chat me on\n",
        "\n",
        "[![facebook](https://img.shields.io/badge/Facebook--blue?style=for-the-badge&labelColor=gray&logo=facebook)](https://www.facebook.com/iyashinouta)\n",
        "\n",
        "[![discord](https://img.shields.io/badge/Discord--7289d9?style=for-the-badge&logo=discord)](https://discord.com/channels/@me/614811143780696077)\n",
        "</details>\n",
        "\n",
        "<font color=\"#CC3E3E\"><font size=\"6\"><b>WARNING</b>\n",
        "\n",
        "* Due to new **[Google Colab ToS][colab_tos]** and **[Official Statement][official_statement]**, I am not responsible for any account suspension due to ToS violation.\n",
        "\n",
        "[colab_tos]: https://research.google.com/colaboratory/faq.html#limitations-and-restrictions\n",
        "[official_statement]: https://twitter.com/thechrisperry/status/1649189902079381505"
      ],
      "metadata": {
        "id": "6YImztr-0nHl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FralfkJAvw01",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #**Click Run Button Below**\n",
        "#@markdown #***Setup UI***\n",
        "from IPython.display import clear_output\n",
        "import base64\n",
        "\n",
        "#Decode Base64 to Hide & Seek from Colab lol\n",
        "kamisato=base64.b64decode((\"c3RhYmxlLWRpZmZ1c2lvbi0=\").encode('ascii')).decode('ascii')\n",
        "ayaka=base64.b64decode((\"d2VidWk=\").encode('ascii')).decode('ascii')\n",
        "\n",
        "#Install Requirement & Rebuild Error Issue\n",
        "!apt -y update -qq\n",
        "!apt -y install -qq aria2\n",
        "clear_output()\n",
        "!curl -Lo memfix.zip https://github.com/nolanaatama/sd-$ayaka/raw/main/memfix.zip\n",
        "!unzip /content/memfix.zip\n",
        "!apt install -qq libunwind8-dev\n",
        "!dpkg -i *.deb\n",
        "%env LD_PRELOAD=libtcmalloc.so\n",
        "!rm *\n",
        "clear_output()\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!pip install -q torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 torchtext==0.15.1 torchdata==0.6.0 --extra-index-url https://download.pytorch.org/whl/cu118 -U\n",
        "!pip install -q xformers==0.0.18 triton==2.0.0 -U\n",
        "!pip install --upgrade fastapi==0.90.1\n",
        "clear_output()\n",
        "\n",
        "#Setup Web UI\n",
        "!git clone https://github.com/AUTOMATIC1111/stable-diffusion-$ayaka\n",
        "!git clone https://github.com/camenduru/sd-$ayaka-tunnels /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-tunnels\n",
        "!git clone https://github.com/yfszzx/stable-diffusion-$ayaka-images-browser /content/stable-diffusion-$ayaka/extensions/stable-diffusion-$ayaka-images-browser\n",
        "!git clone https://github.com/catppuccin/stable-diffusion-$ayaka /content/stable-diffusion-$ayaka/extensions/catppuccin\n",
        "clear_output()\n",
        "!git clone https://github.com/Coyote-A/ultimate-upscale-for-automatic1111 /content/stable-diffusion-$ayaka/extensions/ultimate-upscale-for-automatic1111\n",
        "!git clone https://github.com/Iyashinouta/sd-colab-commands-browser /content/stable-diffusion-$ayaka/extensions/sd-colab-commands-browser\n",
        "!git clone https://github.com/Iyashinouta/sd-model-downloader /content/stable-diffusion-$ayaka/extensions/sd-model-downloader\n",
        "!git clone https://github.com/camenduru/sd-civitai-browser /content/stable-diffusion-$ayaka/extensions/sd-civitai-browser\n",
        "clear_output()\n",
        "!git clone https://github.com/Mikubill/sd-$ayaka-controlnet /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet\n",
        "!git clone https://github.com/nonnonstop/sd-$ayaka-3d-open-pose-editor /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-3d-open-pose-editor\n",
        "!git clone https://github.com/DominikDoom/stable-diffusion-$ayaka-$ayaka-tagcomplete /content/stable-diffusion-$ayaka/extensions/stable-diffusion-$ayaka-$ayaka-tagcomplete\n",
        "!git clone https://github.com/KohakuBlueleaf/stable-diffusion-$ayaka-$ayaka-locon /content/stable-diffusion-$ayaka/extensions/stable-diffusion-$ayaka-$ayaka-locon\n",
        "clear_output()\n",
        "!git clone https://github.com/AUTOMATIC1111/stable-diffusion-$ayaka-rembg /content/stable-diffusion-$ayaka/extensions/stable-diffusion-$ayaka-rembg\n",
        "!git clone https://github.com/ashen-sensored/stable-diffusion-$ayaka-two-shot /content/stable-diffusion-$ayaka/extensions/stable-diffusion-$ayaka-two-shot\n",
        "!git clone https://github.com/camenduru/openpose-editor /content/stable-diffusion-$ayaka/extensions/openpose-editor\n",
        "!git clone https://github.com/jexom/sd-$ayaka-depth-lib /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-depth-lib\n",
        "clear_output()\n",
        "!mkdir /content/stable-diffusion-$ayaka/models/Lora\n",
        "!mkdir /content/stable-diffusion-$ayaka/models/hypernetworks\n",
        "!mkdir /content/stable-diffusion-$ayaka/extensions/stable-diffusion-$ayaka-aesthetic-gradients\n",
        "!mkdir /content/stable-diffusion-$ayaka/extensions/stable-diffusion-$ayaka-aesthetic-gradients/aesthetic_embeddings\n",
        "\n",
        "!git clone https://github.com/Iyashinouta/config\n",
        "!mv -f /content/config/launch1.py /content/stable-diffusion-$ayaka/\n",
        "!mv -f /content/config/config.json /content/stable-diffusion-$ayaka/\n",
        "!rm -rf /content/config/\n",
        "clear_output()\n",
        "\n",
        "#Requirement Models for ControlNet\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/$ayaka/ControlNet-modules-safetensors/resolve/main/control_canny-fp16.safetensors -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/models -o control_canny-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/$ayaka/ControlNet-modules-safetensors/resolve/main/control_depth-fp16.safetensors -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/models -o control_depth-fp16.safetensors\n",
        "clear_output()\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/$ayaka/ControlNet-modules-safetensors/resolve/main/control_hed-fp16.safetensors -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/models -o control_hed-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/$ayaka/ControlNet-modules-safetensors/resolve/main/control_mlsd-fp16.safetensors -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/models -o control_mlsd-fp16.safetensors\n",
        "clear_output()\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/$ayaka/ControlNet-modules-safetensors/resolve/main/control_normal-fp16.safetensors -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/models -o control_normal-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/$ayaka/ControlNet-modules-safetensors/resolve/main/control_openpose-fp16.safetensors -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/models -o control_openpose-fp16.safetensors\n",
        "clear_output()\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/$ayaka/ControlNet-modules-safetensors/resolve/main/control_scribble-fp16.safetensors -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/models -o control_scribble-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/$ayaka/ControlNet-modules-safetensors/resolve/main/control_seg-fp16.safetensors -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/models -o control_seg-fp16.safetensors\n",
        "clear_output()\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/ControlNet/resolve/main/hand_pose_model.pth -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/annotator/openpose -o hand_pose_model.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/ControlNet/resolve/main/body_pose_model.pth -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/annotator/openpose -o body_pose_model.pth\n",
        "clear_output()\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/ControlNet/resolve/main/dpt_hybrid-midas-501f0c75.pt -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/annotator/midas -o dpt_hybrid-midas-501f0c75.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/ControlNet/resolve/main/mlsd_large_512_fp32.pth -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/annotator/mlsd -o mlsd_large_512_fp32.pth\n",
        "clear_output()\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/ControlNet/resolve/main/mlsd_tiny_512_fp32.pth -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/annotator/mlsd -o mlsd_tiny_512_fp32.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/ControlNet/resolve/main/network-bsds500.pth -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/annotator/hed -o network-bsds500.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/ControlNet/resolve/main/upernet_global_small.pth -d /content/stable-diffusion-$ayaka/extensions/sd-$ayaka-controlnet/annotator/uniformer -o upernet_global_small.pth\n",
        "clear_output()\n",
        "\n",
        "#Setup Model(s)\n",
        "#@markdown Pre Install Checkpoint/Base Models (Uncheck if U don't want to Use/Install it)\n",
        "AOAOKO_PVC_Anime = True #@param {type:\"boolean\"}\n",
        "DeliberateV2_AIO = True #@param {type:\"boolean\"}\n",
        "ChillOutMixNiFP16Fix_Realistic = True #@param {type:\"boolean\"}\n",
        "MeinaPastelV4_Anime = True #@param {type:\"boolean\"}\n",
        "IriPastelV1_Anime = True #@param {type:\"boolean\"}\n",
        "PerfectWorldV2_Realistic = True #@param {type:\"boolean\"}\n",
        "AnythingV5_Anime = True #@param {type:\"boolean\"}\n",
        "LigneClaireV1_Anime = True #@param {type:\"boolean\"}\n",
        "LOFIV21_Realistic = True #@param {type:\"boolean\"}\n",
        "or_Any_URL = \"\" #@param {type:\"string\"}\n",
        "\n",
        "with open(\"models.txt\", \"w\") as f:\n",
        "     if AOAOKO_PVC_Anime:\n",
        "        f.write(\"https://civitai.com/api/download/models/18295\\n\"\n",
        "                \" out=AOAOKO_PVC.safetensors\\n\")\n",
        "     else:\n",
        "          pass\n",
        "     if DeliberateV2_AIO:\n",
        "        f.write(\"https://civitai.com/api/download/models/15236\\n\"\n",
        "                \" out=DeliberateV2.safetensors\\n\")\n",
        "     else:\n",
        "          pass\n",
        "     if ChillOutMixNiFP16Fix_Realistic:\n",
        "        f.write(\"https://civitai.com/api/download/models/11732\\n\"\n",
        "                \" out=ChilloutMix-ni.safetensors\\n\")\n",
        "     else:\n",
        "          pass\n",
        "     if MeinaPastelV4_Anime:\n",
        "        f.write(\"https://civitai.com/api/download/models/27135\\n\"\n",
        "                \" out=MeinaPastelV4.safetensors\\n\")\n",
        "     else:\n",
        "          pass\n",
        "     if IriPastelV1_Anime:\n",
        "        f.write(\"https://civitai.com/api/download/models/37979\\n\"\n",
        "                \" out=IrisPastelV1.ckpt\\n\")\n",
        "     else:\n",
        "          pass\n",
        "     if PerfectWorldV2_Realistic:\n",
        "        f.write(\"https://civitai.com/api/download/models/19084\\n\"\n",
        "                \" out=PerfectWorldV2.safetensors\\n\")\n",
        "     else:\n",
        "          pass\n",
        "     if AnythingV5_Anime:\n",
        "        f.write(\"https://civitai.com/api/download/models/30163\\n\"\n",
        "                 \" out=AnythingV5.safetensors\\n\")\n",
        "     else:\n",
        "          pass\n",
        "     if LigneClaireV1_Anime:\n",
        "        f.write(\"https://civitai.com/api/download/models/4279\\n\"\n",
        "                \" out=LigneClaireV1.safetensors\\n\")\n",
        "     else:\n",
        "          pass\n",
        "     if LOFIV21_Realistic:\n",
        "        f.write(\"https://civitai.com/api/download/models/44882\\n\"\n",
        "                \" out=LOFIV2.1.safetensors\")\n",
        "     else:\n",
        "          pass\n",
        "     if or_Any_URL:\n",
        "        f.write(or_Any_URL)\n",
        "     else:\n",
        "          pass\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M --input-file models.txt -d /content/stable-diffusion-$ayaka/models/Stable-diffusion\n",
        "!rm -rf /content/models.txt\n",
        "clear_output()\n",
        "\n",
        "#Bonus Embedding(s)\n",
        "!rm -rf /content/stable-diffusion-$ayaka/embeddings\n",
        "!git clone https://huggingface.co/nolanaatama/embeddings /content/stable-diffusion-$ayaka/embeddings\n",
        "!clear_output()\n",
        "\n",
        "#Bonus Lora(s)\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/nolanaatama/kdllora/resolve/main/kdllorav15.safetensors -d /content/stable-diffusion-$ayaka/models/Lora -o koreanDollLikeness_V15.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/nolanaatama/tdllora/resolve/main/tdllora.safetensors -d /content/stable-diffusion-$ayaka/models/Lora -o taiwanDollLikeness_V10.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/nolanaatama/jdllora/resolve/main/jdllora.safetensors -d /content/stable-diffusion-$ayaka/models/Lora -o japaneseDollLikeness_V10.safetensors\n",
        "clear_output()\n",
        "\n",
        "#qwertytest\n",
        "%cd /content/stable-diffusion-$ayaka\n",
        "!git checkout 0cc0ee1\n",
        "clear_output()\n",
        "\n",
        "import os\n",
        "from google.colab.output import eval_js\n",
        "os.environ['colab_url'] = eval_js(\"google.colab.kernel.proxyPort(7860, {'cache': false})\")\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "clear_output()\n",
        "\n",
        "!python launch1.py\n",
        "!rm -rf /content/stable-diffusion-$ayaka/launch1.py\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #***(ADDITIONAL) Link to Your Google Drive***\n",
        "#@markdown if U want to save Outputs to your Google Drive, Run This\n",
        "\n",
        "#@markdown elif have Bug (ex: not saved to Google Drive), Relaunch (this cell)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/stable-diffusion-$ayaka/outputs')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "1aN9NpxW8ODU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #***Launch Web UI***\n",
        "#@markdown if stopped, have bug or full RAM/VRAM usage, Relaunch (this cell) only\n",
        "\n",
        "!COMMANDLINE_ARGS=\"--listen --no-half-vae --disable-nan-check --remotemoe --allow-code --xformers --force-enable-xformers --enable-insecure-extension-access --disable-console-progressbars --gradio-queue --multiple\" python launch.py"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Y0eZrJ0S3N_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ignore This**\n",
        "~Just for Testing, Dont Use it~"
      ],
      "metadata": {
        "id": "3P3y7QznxKjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #***Launch Web UI Using CPU Only***\n",
        "%cd /content/stable-diffusion-$ayaka\n",
        "!COMMANDLINE_ARGS=\"--share --skip-torch-cuda-test --enable-insecure-extension-access --disable-console-progressbars --deepdanbooru --gradio-queue\" REQS_FILE=\"requirements.txt\" python launch.py"
      ],
      "metadata": {
        "id": "PGR0z_0tPkHF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN0aDXTsiwW9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ## LoRA Training (Duplicate from Hakuon)\n",
        "import os\n",
        "import html\n",
        "import time\n",
        "import textwrap\n",
        "import yaml\n",
        "import zipfile\n",
        "import shutil\n",
        "from IPython.utils import capture\n",
        "from subprocess import getoutput\n",
        "from google.colab import drive\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# List\n",
        "installVae = []\n",
        "\n",
        "# huggingface token for download\n",
        "user_token = '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'\n",
        "user_header = f\"\\\"Authorization: Bearer {user_token}\\\"\"\n",
        "\n",
        "#@markdown ## General Config\n",
        "#@markdown <small><font color=gray> **HINT**: `LoRA` or `Native Training`? you can specify it here. </small><br>\n",
        "mode = \"LoRA\" #@param [\"LoRA\", \"native-training\"]\n",
        "lowram = True #@param {type:\"boolean\"}\n",
        "output_to_drive = True #@param {'type':'boolean'}\n",
        "install_xformers = True #@param {'type':'boolean'}\n",
        "\n",
        "# Define Variable\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "tools_dir = os.path.join(root_dir, \"kohya-trainer/tools\")\n",
        "finetune_dir = os.path.join(root_dir, \"kohya-trainer/finetune\")\n",
        "training_dir =  os.path.join(root_dir, \"training_dir\")\n",
        "pretrained_model = os.path.join(root_dir, \"pretrained_model\")\n",
        "vae = os.path.join(root_dir, \"vae\")\n",
        "\n",
        "if output_to_drive:\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount('/content/drive')\n",
        "  training_dir = \"/content/drive/MyDrive/training_dir\"\n",
        "\n",
        "# Accelerate Config\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "project_name = \"lunox\" #@param {'type' : 'string'}\n",
        "if not project_name:\n",
        "  project_name = \"last\"\n",
        "  \n",
        "train_data_dir = \"/content/drive/MyDrive/Lunox/Dataset\" #@param {'type' : 'string'}\n",
        "#@markdown <small><font color=gray> **HINT**: specify this part if your dataset are in `zip` and uploaded somewhere, this will download your dataset and automatically extract them to `train_data_dir` </small><br> \n",
        "dataset_zip_url = \"\" #@param {'type': 'string'}\n",
        "zipfile = \"train_data.zip\"\n",
        "\n",
        "meta_clean = os.path.join(training_dir, \"meta_clean.json\")\n",
        "meta_cap_dd = os.path.join(training_dir, \"meta_cap_dd.json\")\n",
        "meta_cap = os.path.join(training_dir, \"meta_cap.json\")\n",
        "meta_lat = os.path.join(training_dir, \"meta_lat.json\")\n",
        "output_dir = os.path.join(training_dir, \"output\")\n",
        "\n",
        "# V2 Inference\n",
        "inference_url = \"https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/\"\n",
        "\n",
        "# For Dataset Cleaning\n",
        "supported_types = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".caption\", \".npz\", \".txt\", \".json\"]\n",
        "\n",
        "# Make Directory\n",
        "os.chdir(root_dir)\n",
        "if not os.path.exists(repo_dir):\n",
        " !git clone \"https://github.com/Linaqruf/kohya-trainer\" {repo_dir}\n",
        "\n",
        "for dir in [repo_dir, tools_dir, finetune_dir, deps_dir, training_dir, pretrained_model, vae, training_dir]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "def ubuntu_deps(url, name, dst):\n",
        "  with capture.capture_output() as cap:\n",
        "    !wget -q --show-progress {url}\n",
        "    !unzip -j -o {name} -d \"{dst}\"\n",
        "    !dpkg -i {dst}/*\n",
        "    os.remove(name)\n",
        "    shutil.rmtree(dst)\n",
        "    del cap \n",
        "\n",
        "def install_dependencies():\n",
        "  !pip -q install --upgrade -r requirements.txt\n",
        "\n",
        "  if install_xformers:\n",
        "    !pip install -q --pre xformers\n",
        "    !pip install -q --pre triton\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config):\n",
        "    write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "\n",
        "ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\", \"deb-libs.zip\", deps_dir)\n",
        "os.chdir(repo_dir)\n",
        "install_dependencies()\n",
        "\n",
        "#@markdown ##<br> `NEW` Dreambooth Config\n",
        "\n",
        "use_dreambooth_method = False #@param {type: 'boolean'}\n",
        "instance_token = \"sksfrog\" #@param {type: \"string\"}\n",
        "caption_extension = '.txt' #@param {'type':'string'}\n",
        "\n",
        "#@markdown ##<br> Download Pretrained Model\n",
        "modelUrl = \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp32-pruned.safetensors\" #@param {'type': 'string'}\n",
        "#@markdown <small><font color=gray> **HINT**: useful for downloading pretrained model outside huggingface</small><br>\n",
        "modelName = \"anything-v3-fp32-pruned.safetensors\" #@param {'type': 'string'}\n",
        "if not modelName:\n",
        "  modelName = os.path.basename(modelUrl)\n",
        "modelPath = os.path.join(pretrained_model, modelName)\n",
        "\n",
        "#@markdown ##<br> Download VAE (Optional)\n",
        "vaeUrl = [\"\",\n",
        "          \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\",\n",
        "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\",\n",
        "          \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
        "vaeList = [\"none\",\n",
        "           \"anime.vae.pt\",\n",
        "           \"waifudiffusion.vae.pt\",\n",
        "           \"stablediffusion.vae.pt\"]\n",
        "vaeName = \"anime.vae.pt\" #@param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
        "vaePath = os.path.join(vae, vaeName)\n",
        "\n",
        "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
        "\n",
        "def install_model(url, name, is_vae):\n",
        "  ext = \"ckpt\" if url.endswith(\".ckpt\") else \"safetensors\"\n",
        "  if not is_vae:\n",
        "    if url.startswith(\"https://drive.google.com\"):\n",
        "      os.chdir(pretrained_model)\n",
        "      !gdown --fuzzy {url}\n",
        "    elif url.startswith(\"https://huggingface.co/\"):\n",
        "      if '/blob/' in url:\n",
        "        url = url.replace('/blob/', '/resolve/')\n",
        "      !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {pretrained_model} -o {name} {url}\n",
        "    else:\n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {pretrained_model} -o {name} {url}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -o vae/{name} \"{url}\"\n",
        "\n",
        "os.chdir(root_dir)\n",
        "install_model(modelUrl, modelName, False)\n",
        "if vaeName != \"none\":\n",
        "  for vae in installVae:\n",
        "      install_model(vae[1], vae[0], True)\n",
        "\n",
        "# Unzip Dataset if any\n",
        "def download_dataset(url):\n",
        "  if url.startswith(\"/content\"):\n",
        "    !unzip -j -o {url} -d \"{train_data_dir}\"\n",
        "  elif url.startswith(\"https://drive.google.com\"):\n",
        "    os.chdir(root_dir)\n",
        "    !gdown --fuzzy  {url}\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile} {url}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile} {url}\n",
        "\n",
        "if dataset_zip_url:\n",
        "  download_dataset(dataset_zip_url)\n",
        "\n",
        "  !unzip -j -o {os.path.join(root_dir,zipfile)} -d \"{train_data_dir}\"\n",
        "  os.remove(os.path.join(root_dir,zipfile))\n",
        "\n",
        "  files_to_move = (\"meta_cap.json\",\n",
        "                    \"meta_cap_dd.json\",\n",
        "                    \"meta_lat.json\",\n",
        "                    \"meta_clean.json\")\n",
        "\n",
        "  for filename in os.listdir(train_data_dir):\n",
        "    file_path = os.path.join(train_data_dir, filename)\n",
        "    if filename in files_to_move:\n",
        "      if not os.path.exists(file_path):\n",
        "        shutil.move(file_path, training_dir)\n",
        "      else: \n",
        "        os.remove(file_path)\n",
        "\n",
        "# Clean Dataset\n",
        "for item in os.listdir(train_data_dir):\n",
        "    file_ext = os.path.splitext(item)[1]\n",
        "    item_path = os.path.join(train_data_dir, item)\n",
        "    if os.path.isfile(item_path):\n",
        "        if file_ext not in supported_types:\n",
        "            print(f\"Deleting unsupported file {item} from {train_data_dir}\")\n",
        "            os.remove(item_path)\n",
        "    else:\n",
        "        print(f\"Skipping directory {item}\")\n",
        "\n",
        "#@markdown ##<br> Auto-captioning\n",
        "#@markdown <small><font color=gray> **HINT**: this part will be skipped if you have `any` `.caption` or `.txt` inside your `train_data_dir` </small><br> \n",
        "use_wd_tagger = True #@param{type:\"boolean\"}\n",
        "use_blip = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown ##<br> Custom Tag\n",
        "extension = \"txt\" #@param [\"txt\", \"caption\"]\n",
        "custom_tag = \"lunox\" #@param {type:\"string\"}\n",
        "#@markdown Tick this if you want to append custom tag at the end of lines instead\n",
        "append = False #@param {type:\"boolean\"}\n",
        "keep_tokens = 1 #@param {type:\"number\"}\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "\n",
        "if use_wd_tagger:\n",
        "  if not any([filename.endswith(\".txt\") for filename in os.listdir(train_data_dir)]):\n",
        "    !python tag_images_by_wd14_tagger.py \\\n",
        "      \"{train_data_dir}\" \\\n",
        "      --batch_size 8 \\\n",
        "      --repo_id \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\" \\\n",
        "      --thresh 0.35 \\\n",
        "      --caption_extension .txt \\\n",
        "      --max_data_loader_n_workers 2\n",
        "if use_blip:\n",
        "  if not any([filename.endswith(\".caption\") for filename in os.listdir(train_data_dir)]):\n",
        "    !python make_captions.py \\\n",
        "      \"{train_data_dir}\" \\\n",
        "      --batch_size 8 \\\n",
        "      --beam_search \\\n",
        "      --caption_extension .caption \\\n",
        "      --max_data_loader_n_workers 2\n",
        "\n",
        "def add_tag(filename, tag, append):\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "        \t    \t\n",
        "    tag = \", \".join(tag.split())\n",
        "    tag = tag.replace(\"_\", \" \")\n",
        "    \n",
        "    if tag in contents:\n",
        "        return\n",
        "        \n",
        "    if not keep_tokens:\n",
        "      contents = contents.rstrip() + \", \" + tag if append else tag + \", \" + contents\n",
        "    else:\n",
        "      contents = tag + \", \" + contents\n",
        "    \n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "if custom_tag:\n",
        "  if not any([filename.endswith(\".\" + extension) for filename in os.listdir(train_data_dir)]):\n",
        "      for filename in os.listdir(train_data_dir):\n",
        "          if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")):\n",
        "              open(os.path.join(train_data_dir, filename.split(\".\")[0] + \".\" + extension), \"w\").close()\n",
        "\n",
        "  tags = custom_tag.split()\n",
        "\n",
        "  for filename in os.listdir(train_data_dir):\n",
        "      if filename.endswith(\".\" + extension):\n",
        "          for tag in tags:\n",
        "              add_tag(os.path.join(train_data_dir, filename), tag, append)\n",
        "\n",
        "# Create JSON file for Finetuning\n",
        "if use_dreambooth_method:\n",
        "  files = [f for f in os.listdir(train_data_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "  for file in files:\n",
        "      file_path = os.path.join(train_data_dir, file)\n",
        "\n",
        "      with open(file_path, \"r\") as f:\n",
        "          contents = f.read()\n",
        "\n",
        "      contents = html.unescape(contents)\n",
        "      contents = contents.replace(\"_\", \" \")\n",
        "      contents = \", \".join(contents.split(\"\\n\"))\n",
        "\n",
        "      with open(file_path, \"w\") as f:\n",
        "          f.write(contents)\n",
        "else:\n",
        "  if os.path.exists(train_data_dir):\n",
        "    if any(file.endswith('.caption') for file in os.listdir(train_data_dir)):\n",
        "      !python merge_captions_to_metadata.py \\\n",
        "        {train_data_dir} \\\n",
        "        {meta_cap}\n",
        "\n",
        "    if any(file.endswith('.txt') for file in os.listdir(train_data_dir)):\n",
        "      !python merge_dd_tags_to_metadata.py \\\n",
        "        {train_data_dir} \\\n",
        "        {meta_cap_dd}\n",
        "  else:\n",
        "    print(\"train_data_dir does not exist or is not a directory.\")\n",
        "\n",
        "  if os.path.exists(meta_cap):\n",
        "    !python merge_dd_tags_to_metadata.py \\\n",
        "      {train_data_dir} \\\n",
        "      --in_json {meta_cap} \\\n",
        "      {meta_cap_dd}\n",
        "\n",
        "  if os.path.exists(meta_cap_dd):\n",
        "    !python clean_captions_and_tags.py \\\n",
        "      {meta_cap_dd} \\\n",
        "      {meta_clean}\n",
        "  elif os.path.exists(meta_cap):\n",
        "    !python clean_captions_and_tags.py \\\n",
        "      {meta_cap} \\\n",
        "      {meta_clean}\n",
        "\n",
        "#@markdown ##<br> LoRA Config\n",
        "\n",
        "network_dim = 128 #@param {'type':'number'}\n",
        "network_alpha = 128 #@param {'type':'number'}\n",
        "#@markdown `network_weights` can be specified to resume training.\n",
        "network_weights = \"\" #@param {'type':'string'}\n",
        "unet_lr = 1e-4 #@param {'type':'number'}\n",
        "text_encoder_lr = 5e-5 #@param {'type':'number'}\n",
        "\n",
        "#@markdown ##<br> Optimizer Config\n",
        "#@markdown `AdamW8bit` was the old `--use_8bit_adam`. Use `DAdaptation` if you find it hard to set optimizer hyperparameter right.\n",
        "optimizer_type = \"AdamW8bit\" #@param [\"AdamW\", \"AdamW8bit\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"DAdaptation\", \"AdaFactor\"]\n",
        "#@markdown Additional arguments for optimizer, e.g: `\"decouple=True weight_decay=0.01 betas=0.9,0.999 ...\"`\n",
        "optimizer_args = \"\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown <small><font color=gray> **HINT**: for LoRA if you specify both `--unet_lr` and `--text_encoder_lr` you don't need this, however it's still recorded to metadata</small><br>  \n",
        "learning_rate = 2e-6 #@param {type:\"number\"}\n",
        "lr_scheduler = \"constant\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
        "lr_warmup_steps = 250 #@param {'type':'number'}\n",
        "#@markdown You can define `num_cycles` value for `cosine_with_restarts` or `power` value for `polynomial` in the field below.\n",
        "lr_scheduler_args = 1 #@param {'type':'number'}\n",
        "\n",
        "#@markdown ##<br> Training Config\n",
        "#@markdown <small><font color=gray> **HINT**: specify `v2` if you train on SDv2 base Model, with `v2_parameterization` for SDv2 768 Model</small><br>  \n",
        "v2 = False #@param{type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "resolution = 768 #@param {type:\"slider\", min:512, max:768, step:128}\n",
        "flip_aug = False #@param{type:\"boolean\"}\n",
        "#@markdown Read [Diffusion With Offset Noise](https://www.crosslabs.org//blog/diffusion-with-offset-noise), in short, you can control and easily generating darker or light images by offset the noise when fine-tuning the model. Set to `0` by default, recommended value: `0.1`\n",
        "noise_offset = 0.1 #@param {type:\"number\"}\n",
        "#@markdown <small><font color=gray> **HINT**: try lowering `train_batch_size` if you do native training, around 4 batch sizes\n",
        "train_batch_size = 2 #@param {type:\"number\"}\n",
        "\n",
        "s = getoutput('nvidia-smi')\n",
        "if 'T4' in s:\n",
        "  if mode == \"native-training\" and train_batch_size > 4:\n",
        "    train_batch_size = 4\n",
        "  elif mode == \"LoRA\" and train_batch_size > 8:\n",
        "    train_batch_size = 8\n",
        "\n",
        "max_train_type = \"max_train_epochs\" #@param [\"max_train_steps\", \"max_train_epochs\"]\n",
        "max_train_type_value = 10 #@param {type:\"number\"}\n",
        "dataset_repeats = 10 #@param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_n_epochs_type = \"save_n_epoch_ratio\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
        "save_n_epochs_type_value = 3 #@param {type:\"number\"}\n",
        "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
        "clip_skip = 2 #@param {type:\"number\"}\n",
        "logging_dir = \"/content/training_dir/logs\"\n",
        "additional_argument = \"--shuffle_caption --xformers\" #@param {type:\"string\"}\n",
        "print_hyperparameter = True\n",
        "\n",
        "# Absolute value\n",
        "min_bucket_reso = 320 if resolution > 640 else 256\n",
        "max_bucket_reso = 1280 if resolution > 640 else 1024\n",
        "prior_loss_weight = 1.0\n",
        "\n",
        "# Dreambooth Config\n",
        "\n",
        "dreambooth_dir = f\"{dataset_repeats}_{instance_token}\"\n",
        "dreambooth_data_dir = os.path.join(train_data_dir, dreambooth_dir)\n",
        "reg_data_dir = os.path.join(os.path.dirname(train_data_dir), \"reg_data\")\n",
        "\n",
        "if use_dreambooth_method:\n",
        "  os.makedirs(dreambooth_data_dir, exist_ok=True)\n",
        "  os.makedirs(reg_data_dir, exist_ok=True)\n",
        "\n",
        "  for dataset in os.listdir(train_data_dir):\n",
        "      file_ext = os.path.splitext(dataset)[1]\n",
        "      if file_ext in supported_types:\n",
        "          source = os.path.join(train_data_dir, dataset)\n",
        "          destination = os.path.join(dreambooth_data_dir, dataset)\n",
        "          shutil.move(source, destination)\n",
        "else:\n",
        "  if os.path.exists(dreambooth_data_dir):\n",
        "    for dataset in os.listdir(dreambooth_data_dir):\n",
        "        source = os.path.join(dreambooth_data_dir, dataset)\n",
        "        destination = os.path.join(train_data_dir, dataset)\n",
        "        shutil.move(source, destination)\n",
        "    if not os.listdir(dreambooth_data_dir):\n",
        "        shutil.rmtree(dreambooth_data_dir)\n",
        "\n",
        "# V2 Config\n",
        "if v2 and not v_parameterization:\n",
        "  inference_url += \"v2-inference.yaml\"\n",
        "if v2 and v_parameterization:\n",
        "  inference_url += \"v2-inference-v.yaml\"\n",
        "\n",
        "# Download config\n",
        "try:\n",
        "  if v2:\n",
        "    !wget -c {inference_url} -O {training_dir}/{project_name}.yaml\n",
        "    print(\"File successfully downloaded\")\n",
        "except:\n",
        "  print(\"There was an error downloading the file. Please check the URL and try again.\")\n",
        "\n",
        "# Max Resolution\n",
        "if resolution == 512:\n",
        "  max_resolution = \"512,512\"\n",
        "elif resolution == 640:\n",
        "  max_resolution = \"640,640\"\n",
        "else:\n",
        "  max_resolution = \"768,768\"\n",
        "\n",
        "# Run script to prepare buckets and latent\n",
        "if not use_dreambooth_method:\n",
        "  if not os.path.exists(meta_lat):\n",
        "    bucket_latents=f\"\"\"\n",
        "    python prepare_buckets_latents.py \\\n",
        "      {train_data_dir} \\\n",
        "      {meta_clean} \\\n",
        "      {meta_lat} \\\n",
        "      {modelPath} \\\n",
        "      {\"--v2\" if v2 else \"\"} \\\n",
        "      {\"--flip_aug\" if flip_aug else \"\"} \\\n",
        "      {\"--min_bucket_reso \" + format(320) if resolution != 512 else \"--min_bucket_reso \" + format(256)} \\\n",
        "      {\"--max_bucket_reso \" + format(1280) if resolution != 512 else \"--max_bucket_reso \" + format(1024)} \\\n",
        "      {\"--batch_size \" + format(8)} \\\n",
        "      {\"--max_resolution \" + format(max_resolution)} \\\n",
        "      --mixed_precision no\n",
        "      \"\"\"\n",
        "    f = open(\"./bucket_latents.sh\", \"w\")\n",
        "    f.write(bucket_latents)\n",
        "    f.close()\n",
        "    !chmod +x ./bucket_latents.sh\n",
        "    !./bucket_latents.sh\n",
        "\n",
        "# Start Training\n",
        "os.chdir(repo_dir)\n",
        "train_command=f\"\"\"\n",
        "accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=8 {f\"{repo_dir}/train_network.py\" if mode == \"LoRA\" else (f\"{repo_dir}/fine_tune.py\" if not use_dreambooth_method else f\"{repo_dir}/train_db.py\")} \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  {\"--output_name=\" + project_name if project_name else \"\"} \\\n",
        "  --pretrained_model_name_or_path={modelPath} \\\n",
        "  {\"--vae=\" + vaePath if vaePath else \"\"} \\\n",
        "  --train_data_dir={train_data_dir} \\\n",
        "  {\"--reg_data_dir=\" + reg_data_dir if use_dreambooth_method else \"\"} \\\n",
        "  {\"--in_json=\" + meta_lat if not use_dreambooth_method else \"\"} \\\n",
        "  --output_dir={output_dir} \\\n",
        "  {\"--network_dim=\" + format(network_dim) if mode == \"LoRA\" else \"\"} \\\n",
        "  {\"--network_alpha=\" + format(network_alpha) if mode == \"LoRA\" else \"\"} \\\n",
        "  {\"--network_module=networks.lora\" if mode == \"LoRA\" else \"\"} \\\n",
        "  {(\"--network_weights=\" + network_weights if network_weights else \"\") if mode == \"LoRA\" else \"\"} \\\n",
        "  {(\"--unet_lr=\" + format(unet_lr) if unet_lr else \"\") if mode == \"LoRA\" else \"\"} \\\n",
        "  {(\"--text_encoder_lr=\" + format(text_encoder_lr) if text_encoder_lr else \"\") if mode == \"LoRA\" else \"\"} \\\n",
        "  --optimizer_type={optimizer_type} \\\n",
        "  {\"--optimizer_args=\" + optimizer_args if optimizer_args else \"\"} \\\n",
        "  --learning_rate={learning_rate} \\\n",
        "  --lr_scheduler={lr_scheduler} \\\n",
        "  {\"--lr_warmup_steps=\" + format(lr_warmup_steps) if lr_warmup_steps else \"\"} \\\n",
        "  {\"--lr_scheduler_num_cycles=\" + format(lr_scheduler_args) if lr_scheduler == \"cosine_with_restarts\" else \"\"} \\\n",
        "  {\"--lr_scheduler_power=\" + format(lr_scheduler_args) if lr_scheduler == \"polynomial\" else \"\"} \\\n",
        "  {\"--dataset_repeats=\" + format(dataset_repeats) if not use_dreambooth_method else \"\"} \\\n",
        "  --resolution={resolution} \\\n",
        "  {\"--enable_bucket\" if use_dreambooth_method else \"\"} \\\n",
        "  {\"--keep_tokens=\" + format(keep_tokens) if custom_tag else \"\"} \\\n",
        "  {\"--min_bucket_reso=\" + format(min_bucket_reso) if use_dreambooth_method else \"\"} \\\n",
        "  {\"--max_bucket_reso=\" + format(max_bucket_reso) if use_dreambooth_method else \"\"} \\\n",
        "  {\"--caption_extension=\" + caption_extension if use_dreambooth_method else \"\"} \\\n",
        "  {\"--cache_latents\" if use_dreambooth_method else \"\"} \\\n",
        "  {\"--prior_loss_weight=\" + format(prior_loss_weight) if use_dreambooth_method else \"\"} \\\n",
        "  {\"--lowram\" if lowram else \"\"} \\\n",
        "  {\"--noise_offset=\" + format(noise_offset) if noise_offset > 0 else \"\"} \\\n",
        "  --mixed_precision={mixed_precision} \\\n",
        "  --save_precision={save_precision} \\\n",
        "  {\"--save_every_n_epochs=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_every_n_epochs\" else \"\"} \\\n",
        "  {\"--save_n_epoch_ratio=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_n_epoch_ratio\" else \"\"} \\\n",
        "  --save_model_as={save_model_as} \\\n",
        "  --train_batch_size={train_batch_size} \\\n",
        "  {\"--max_token_length=\" + format(225)} \\\n",
        "  {\"--max_train_epochs=\" + format(max_train_type_value) if max_train_type == \"max_train_epochs\" else \"\"} \\\n",
        "  {\"--max_train_steps=\" + format(max_train_type_value) if max_train_type == \"max_train_steps\" else \"\"} \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if not v2 else \"\"} \\\n",
        "  --logging_dir={logging_dir} \\\n",
        "  --log_prefix={project_name} \\\n",
        "  {additional_argument}\n",
        "  \"\"\"\n",
        "\n",
        "debug_params = [\"mode\",\n",
        "                \"use_dreambooth_method\",\n",
        "                \"lowram\",\n",
        "                \"v2\",\n",
        "                \"v_parameterization\",\n",
        "                \"project_name\",\n",
        "                \"modelPath\",\n",
        "                \"vaePath\",\n",
        "                \"train_data_dir\",\n",
        "                \"reg_data_dir\" if use_dreambooth_method else \"\",\n",
        "                \"meta_lat\" if not use_dreambooth_method else \"\",\n",
        "                \"output_dir\",\n",
        "                \"network_dim\" if mode == \"LoRA\" else \"\" ,\n",
        "                \"network_alpha\" if mode == \"LoRA\" else \"\" ,\n",
        "                \"network_weights\" if mode == \"LoRA\" else \"\",\n",
        "                \"unet_lr\" if mode == \"LoRA\" else \"\",\n",
        "                \"text_encoder_lr\" if mode == \"LoRA\" else \"\",\n",
        "                \"optimizer_type\", \n",
        "                \"optimizer_args\",\n",
        "                \"learning_rate\",\n",
        "                \"lr_scheduler\",\n",
        "                \"lr_warmup_steps\", \n",
        "                \"lr_scheduler_args\",\n",
        "                \"keep_tokens\" if custom_tag else \"\",\n",
        "                \"dataset_repeats\" if not use_dreambooth_method else \"\",\n",
        "                \"min_bucket_reso\" if use_dreambooth_method else \"\",\n",
        "                \"max_bucket_reso\" if use_dreambooth_method else \"\",\n",
        "                \"resolution\",                \n",
        "                \"caption_extension\" if use_dreambooth_method else \"\",\n",
        "                \"noise_offset\",\n",
        "                \"prior_loss_weight\" if use_dreambooth_method else \"\",\n",
        "                \"mixed_precision\",\n",
        "                \"save_precision\",\n",
        "                \"save_n_epochs_type\",\n",
        "                \"save_n_epochs_type_value\",\n",
        "                \"save_model_as\",\n",
        "                \"train_batch_size\",\n",
        "                \"max_train_type\",\n",
        "                \"max_train_type_value\",\n",
        "                \"clip_skip\",\n",
        "                \"logging_dir\",\n",
        "                \"additional_argument\"]\n",
        "\n",
        "if print_hyperparameter:\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Hyperparameter\", \"Value\"]\n",
        "    for params in debug_params:\n",
        "        if params != \"\":\n",
        "            if globals()[params] == \"\":\n",
        "                value = \"False\"\n",
        "            else:\n",
        "                value = globals()[params]\n",
        "            table.add_row([params, value])\n",
        "    table.align = \"l\"\n",
        "    print(table)\n",
        "\n",
        "    arg_list = train_command.split()\n",
        "    mod_train_command = {'command': arg_list}\n",
        "    \n",
        "    if mode == \"LoRA\":\n",
        "      # save the YAML string to a file\n",
        "      with open((f'{training_dir}/dreambooth_lora_cmd.yaml' if use_dreambooth_method else f'{training_dir}/finetune_lora_cmd.yaml'), 'w') as f:\n",
        "          yaml.dump(mod_train_command, f)\n",
        "    else:\n",
        "      with open((f'{training_dir}/dreambooth_cmd.yaml' if use_dreambooth_method else f'{training_dir}/finetune_cmd.yaml'), 'w') as f:\n",
        "            yaml.dump(mod_train_command, f)\n",
        "\n",
        "f = open(\"./train.sh\", \"w\")\n",
        "f.write(train_command)\n",
        "f.close()\n",
        "!chmod +x ./train.sh\n",
        "!./train.sh"
      ]
    }
  ]
}